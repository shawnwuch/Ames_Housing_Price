{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Information regarding Ames Housing Dataset compiled by Dean De Cock.\n",
    "\n",
    "Reference: De Cock, D. (2011). Ames, Iowa: Alternative to the Boston housing data as an end of semester regression project. Journal of Statistics Education, 19(3).\n",
    "\n",
    "Download Link: https://www.openintro.org/stat/data/?data=ames\n",
    "\n",
    "The Ames Housing Dataset has a total of 80 features (23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables) for each property sale, which is more comprehensive than the Boston Housing Dataset (http://lib.stat.cmu.edu/datasets/boston), which only has 506 observations and 14 variables. The strength of using such a dataset is that we will **have more features to make potentially better prediction for the *SalePrice*, which is our response variable**. But we also need to be aware that **there may be more missing data**, and also we would need to **pay extra attention to the normalization of features** and **to the correlation between features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin by setting up the Python environment\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.warn = warn\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p, inv_boxcox1p\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import robust_scale\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./data\"]).decode(\"utf8\")) #check the files available in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the datasets in pandas dataframe\n",
    "df_full = pd.read_csv('./data/Ames.csv')\n",
    "df_full.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the source dataset from De Cock, two additional columns (Order and PID) were added in the downloaded csv. Since both of them are not features of a propertiy, we can safely remove them before further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.drop(['Order', 'PID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.head(10) # double check if the columns were deleted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.info() # show the data type and spot missing values\n",
    "print(\"There are \", len(df_full.dtypes[df_full.dtypes != \"object\"].index), \"numeric features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.describe() # Summarize the numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary we can see there are 37 numeric features and some missing values. For example, **Pool.QC, Misc.Feature, and Alley** have much fewer entries than the other features. This seems reasonable because many houses do not have a pool or an alley. Before moving foward to feature engineering we can visualize the dataset first to see if there is any obvious correlations between the response variable (SalePrice) and the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "corrmat = df_full.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(corrmat, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix we can see that **Garage.Cars** and **Garage.Area** are highly correlated. This makes sense because more cars can be parked in a larger garage. We may choose one of them for our Automated Valuation Models (AVM). Additionally, **Garage.Yr.Blt** and **Year.Built**, **X1st.Flr.SF** and **Total.Bsmt.SF**, as well as **TotRms.AbvGrd** (Total rooms above grade--does not include bathrooms) and **Gr.Liv.Area** (Above grade/ground living area square feet) are all reasonably correlated. These observations also make sense since a house and its garage are usually built in the same time, a house with a large basement usually has a spacious first floor (because 1st floor area is typically larger than basement area),and more living area above grade means there is likely to be more rooms in a house.\n",
    "\n",
    "Next we take a closer look at features that correlate strongly with SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SalePriceCorr = df_full.corr().abs()['SalePrice']\n",
    "print(SalePriceCorr.sort_values(kind=\"quicksort\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use scatter and box plots to know more about the trends of SalePrice with respect to some of the highly correlated features, such as **Overall.Qual, Gr.Liv.Area, Garage.Cars, Total.Bsmt.SF, Year.Built, Full.Bath**. Note that I skipped Garage.Cars and X1st.Flr.SF because their correlation with Garage.Area and Total.Bsmt.SF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bivariate analysis of SalePrice and continuous variables\n",
    "correlated_features = ('Gr.Liv.Area', 'Total.Bsmt.SF', 'Year.Built')\n",
    "for feature in correlated_features:\n",
    "    data = pd.concat([df_full['SalePrice'], df_full[feature]], axis=1)\n",
    "    data.plot.scatter(x=feature, y='SalePrice', ylim=(0,800000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots of SalePrice and ordinal discrete variables\n",
    "correlated_features = ('Overall.Qual', 'Garage.Cars', 'Full.Bath')\n",
    "for feature in correlated_features:\n",
    "    data = pd.concat([df_full['SalePrice'], df_full[feature]], axis=1)\n",
    "    f, ax = plt.subplots(figsize=(8, 6))\n",
    "    fig = sns.boxplot(x=feature, y=\"SalePrice\", data=data)\n",
    "    fig.axis(ymin=0, ymax=800000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key observations\n",
    "1. SalePrice increases with Gr.Liv.Area (linearly with a higher uncertainty for houses with large living area).\n",
    "\n",
    "2. SalePrice increases with Total.Bsmt.SF (looks like an exponential trend, e.g., y=x^2, but with two obvious outliers for >5000 squared ft basement and uncertainty for houses without a basement, i.e., 0 squared ft). \n",
    "\n",
    "3. Higher fluctuation of SalePrice for house built in recent decades (after ~1990).\n",
    "\n",
    "4. The higher Overall.Qual is the higher the SalePrice. The order of the quality has a meaning to it.\n",
    "\n",
    "5. Houses with a 3-car garage and 3 full bathrooms tend to have a higher SalePrice. Note that there is almost no missing data for these two features.\n",
    "\n",
    "Next I want to have a look at the distribution of the SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SalePrice is the response variable we need to predict. Do some analysis on it first.\n",
    "sns.distplot(df_full['SalePrice'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df_full['SalePrice'])\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. \\n$\\mu=$ {:.2f} \\n$\\sigma=$ {:.2f}'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('PDF')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Create the Normal Probability Plot to check if SalePrice is normally distributed\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_full['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a Box-Cox (or log) tranformation to make the SalePrice more normally distributed. It will help later on when training our (linear) models.\n",
    "\n",
    "*Reference for Box-Cox transformation: https://www.itl.nist.gov/div898/handbook/eda/section3/boxcoxno.htm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full[\"SalePrice\"] = np.log1p(df_full[\"SalePrice\"])\n",
    "\n",
    "lamda_SP = boxcox_normmax(df_full[\"SalePrice\"]) # the lambda factor\n",
    "df_full[\"SalePrice\"] = boxcox1p(df_full[\"SalePrice\"], lamda_SP)\n",
    "\n",
    "# SalePrice is the response variable we need to predict. Do some analysis on it first.\n",
    "sns.distplot(df_full[\"SalePrice\"] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df_full[\"SalePrice\"])\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. \\n$\\mu=$ {:.2f} \\n$\\sigma=$ {:.2f}'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('PDF')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Create the Normal Probability Plot to check if SalePrice is normally distributed\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_full[\"SalePrice\"], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a basic understanding of the data we start to handle the missing data and build the AVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the Response (SalePrice) from Features\n",
    "Response = df_full[\"SalePrice\"]\n",
    "df_full.drop([\"SalePrice\"], axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "1. Imputing missing values in the features sequentially (be aware of duplicates if there is any)\n",
    "\n",
    "2. Transforming numerical variables into categorical variables (e.g., Mo.Sold, MS.SubClass etc.)\n",
    "\n",
    "3. Label Encoding some categorical variables that may have information in their ordering\n",
    "\n",
    "4. Box Cox / log transformation of skewed features\n",
    "\n",
    "5. Normalization of numerical features to avoid ill-conditioning\n",
    "\n",
    "6. Getting dummy variables for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_na = (df_full.isnull().sum() / len(df_full)) * 100  # calculate the ratio of missing data\n",
    "df_full_na = df_full_na.drop(df_full_na[df_full_na == 0].index).sort_values(ascending=False) # drop features w/o any missing data\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :df_full_na})\n",
    "print(missing_data) # list the missing ratio in descending order\n",
    "print(len(missing_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imputing missing values in the features sequentially\n",
    "* Pool.QC: NA means \"No Pool\".\n",
    "* Misc.Feature: NA means \"no misc feature\".\n",
    "* Alley: NA means \"no alley access\".\n",
    "* Fence: NA means \"no fence\".\n",
    "* Fireplace.Qu : NA means \"no fireplace\".\n",
    "* Lot.Frontage : Assuming that the lot in front of a house does not vary by much within one neighborhood, I will use the median Lot.Frontage in the neighborhood to replace missing values.\n",
    "* Garage.Yr.Blt, Garage.Area and Garage.Cars : Replacing missing data with 0 for houses with no garage.\n",
    "* Garage.Type, Garage.Finish, Garage.Qual and Garage.Cond : NA means \"no garage\".\n",
    "* Bsmt.Qual, Bsmt.Cond, Bsmt.Exposure, Bsmt.Fin.Type1 and Bsmt.Fin.Type.2 : For these categorical features NA means that there is no basement.\n",
    "* BsmtFin.SF.1, BsmtFin.SF.2, Bsmt.Unf.SF, Total.Bsmt.SF, Bsmt.Full.Bath and Bsmt.Half.Bath : Missing values are likely 0 for these numeric features of a house without a basement.\n",
    "* Mas.Vnr.Area and Mas.Vnr.Type : NA means no masonry veneer for the houses, so we will fill 0 for the area and None for the type.\n",
    "* Electrical : This categorical feature has one missing value. We will fill it in with the mode (SBrkr:\tStandard Circuit Breakers & Romex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"Pool.QC\"] = df_full[\"Pool.QC\"].fillna(\"None\")\n",
    "\n",
    "# double check if all Nan are dealt\n",
    "print(df_full[\"Pool.QC\"].isnull().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"Misc.Feature\"] = df_full[\"Misc.Feature\"].fillna(\"None\")\n",
    "df_full[\"Alley\"] = df_full[\"Alley\"].fillna(\"None\")\n",
    "df_full[\"Fence\"] = df_full[\"Fence\"].fillna(\"None\")\n",
    "df_full[\"Fireplace.Qu\"] = df_full[\"Fireplace.Qu\"].fillna(\"None\")\n",
    "\n",
    "# double check if all Nan are dealt\n",
    "print(df_full[\"Misc.Feature\"].isnull().sum())\n",
    "print(df_full[\"Alley\"].isnull().sum())\n",
    "print(df_full[\"Fence\"].isnull().sum())\n",
    "print(df_full[\"Fireplace.Qu\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n",
    "df_full[\"Lot.Frontage\"] = df_full.groupby(\"Neighborhood\")[\"Lot.Frontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))\n",
    "\n",
    "# double check if all Nan are dealt\n",
    "print(df_full[\"Lot.Frontage\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still missing 3 entries, try analyze further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full[df_full[\"Lot.Frontage\"].isnull()][\"Neighborhood\"]) # See which neighborhoods these 3 properties are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full.ix[df_full[\"Neighborhood\"] == \"GrnHill\", \"Lot.Frontage\"])\n",
    "print(df_full.ix[df_full[\"Neighborhood\"] == \"Landmrk\", \"Lot.Frontage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houses in \"GrnHill\" and \"Landmrk\" neighborhood all miss the Lot.Frontage, thereby, fill it with median of all Lot.Frontage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"Lot.Frontage\"] = df_full[\"Lot.Frontage\"].fillna(df_full[\"Lot.Frontage\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check if missing values are input correctly\n",
    "print(df_full.ix[df_full[\"Neighborhood\"] == \"GrnHill\", \"Lot.Frontage\"])\n",
    "print(df_full.ix[df_full[\"Neighborhood\"] == \"Landmrk\", \"Lot.Frontage\"])\n",
    "\n",
    "# double check if all Nan are dealt\n",
    "print(df_full[\"Lot.Frontage\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing garage data with 'None' or 0 for houses with no garage\n",
    "for col in ('Garage.Type', 'Garage.Finish', 'Garage.Qual', 'Garage.Cond'):\n",
    "    df_full[col] = df_full[col].fillna('None')\n",
    "    print(df_full[col].isnull().sum())\n",
    "    \n",
    "for col in ('Garage.Yr.Blt', 'Garage.Area', 'Garage.Cars'):\n",
    "    df_full[col] = df_full[col].fillna(0)\n",
    "    print(df_full[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Bsmt missing data\n",
    "for col in (\"BsmtFin.SF.1\", \"BsmtFin.SF.2\", \"Bsmt.Unf.SF\", \"Total.Bsmt.SF\", \"Bsmt.Full.Bath\", \"Bsmt.Half.Bath\"):\n",
    "    df_full[col] = df_full[col].fillna(0)\n",
    "    print(df_full[col].isnull().sum())\n",
    "    \n",
    "for col in ('Bsmt.Qual', 'Bsmt.Cond', 'Bsmt.Exposure', 'BsmtFin.Type.1', 'BsmtFin.Type.2'):\n",
    "    df_full[col] = df_full[col].fillna('None')\n",
    "    print(df_full[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Masonry veneer data\n",
    "df_full[\"Mas.Vnr.Type\"] = df_full[\"Mas.Vnr.Type\"].fillna(\"None\")\n",
    "df_full[\"Mas.Vnr.Area\"] = df_full[\"Mas.Vnr.Area\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['Electrical'] = df_full['Electrical'].fillna(df_full['Electrical'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check if there is still any missing value or Nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_na = (df_full.isnull().sum() / len(df_full)) * 100  # calculate the ratio of missing data\n",
    "df_full_na = df_full_na.drop(df_full_na[df_full_na == 0].index).sort_values(ascending=False)\n",
    "df_full_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transforming numerical variables into categorical variables \n",
    "Transform a part of the numerical variables to categorical because they merely represent different classes without any ordering. Later I will apply **LabelEncoder for ordinal encoding** or **pd.get_dummies for one-hot encoding** to these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform MSSubClass (which identifies the type of dwelling involved in the sale) \n",
    "# and Overall.Cond from numerical to categorical.\n",
    "df_full['MS.SubClass'] = df_full['MS.SubClass'].astype(str)\n",
    "df_full['Overall.Cond'] = df_full['Overall.Cond'].astype(str)\n",
    "\n",
    "# #Year and month sold are transformed into categorical features.\n",
    "df_full['Yr.Sold'] = df_full['Yr.Sold'].astype(str) # All properties were sold between 2006-2010, so only 4 levels\n",
    "df_full['Mo.Sold'] = df_full['Mo.Sold'].astype(str) # All properties were sold between Jan. to Dec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Label Encoding categorical variables with information in their ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoder to features associated with quality/condition of living and have ordinal levels\n",
    "Ordered_CategoricalFeatures = ('Fireplace.Qu', 'Bsmt.Qual', 'Bsmt.Cond', 'Garage.Qual', 'Garage.Cond', \n",
    "        'Exter.Qual', 'Exter.Cond','Heating.QC', 'Pool.QC', 'Kitchen.Qual', 'BsmtFin.Type.1', \n",
    "        'BsmtFin.Type.2', 'Functional', 'Fence', 'Bsmt.Exposure', 'Garage.Finish', 'Land.Slope',\n",
    "        'Lot.Shape', 'Paved.Drive', 'Street', 'Alley', 'Central.Air', 'MS.SubClass', 'Overall.Cond')\n",
    "\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for feature in Ordered_CategoricalFeatures:\n",
    "    le = LabelEncoder() \n",
    "    le.fit(list(df_full[feature].values)) \n",
    "    df_full[feature] = le.transform(list(df_full[feature].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are still\", len(df_full.dtypes[df_full.dtypes == \"object\"].index), \"categorical variables.\")\n",
    "print(df_full.dtypes[df_full.dtypes == \"object\"].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Box Cox / log transformation of skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_vars = df_full.dtypes[df_full.dtypes != \"object\"].index\n",
    "print(\"There are\", len(numeric_vars), \"numerical variables now.\")\n",
    "\n",
    "# Check the skewness of all continuous variables\n",
    "skewed_vars = df_full[numeric_vars].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Skew' :skewed_vars})\n",
    "skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skewness.loc[skewness['Skew'].abs() > 0.5] # select features that are more than moderately skewed\n",
    "skewed_features = skewness.index\n",
    "for feat in skewed_features:\n",
    "    lamda = boxcox_normmax(df_full[feat]+1) # the lambda factor\n",
    "    df_full[feat] = boxcox1p(df_full[feat], lamda)\n",
    "    \n",
    "print(\"There were\", len(skewness), \"skewed numerical features Box-Cox transformed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Normalization of numerical features to avoid ill-conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale by Mean/Max-Min or Median/IQR\n",
    "df_num = df_full.select_dtypes(include=[np.number])\n",
    "# df_norm = (df_num - df_num.mean()) / (df_num.max() - df_num.min())\n",
    "# df_full[df_norm.columns] = df_norm\n",
    "\n",
    "df_norm = robust_scale(df_num, copy=True)\n",
    "df_full[df_num.columns] = df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Getting dummy variables for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full.shape)\n",
    "\n",
    "df_full = pd.get_dummies(df_full)\n",
    "print(df_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df_full, Response, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dimension of full dataset is \", df_full.shape)\n",
    "print(\"The dimension of training set is \", X_train.shape)\n",
    "print(\"The dimension of test set is \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a training set and a test set. We can begining training the different models and compare their performance. We will using cross-validation for hyperparameter selection.\n",
    "\n",
    "## Modeling\n",
    "\n",
    "Select one of following models and do the fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*LASSO Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha =0.0005, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Random Forest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(max_depth=10, random_state=2, n_estimators=300)\n",
    "# Nrange = np.arange(10, 20, 10)\n",
    "# Drange = np.arange(4, 11)\n",
    "# pars = {'n_estimators': Nrange, 'criterion': ['mse'], 'max_depth': Drange, 'max_features': ['sqrt'], 'n_jobs': [-1]}\n",
    "# model.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Elastic Net (Ridge + LASSO)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Kernal Ridge Regression (http://scikit-learn.org/stable/modules/kernel_ridge.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelRidge(alpha=0.9, kernel='polynomial', degree=2, coef0=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Epsilon-Support Vector Regression (http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SVR(kernel='linear') # Slow, O(m_obs^2 * n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gradient Boosting Regressor (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the model prediction with actual sale price (inverted back to real prices)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(x=inv_boxcox1p(model.predict(X_test), lamda_SP), y=inv_boxcox1p(Y_test, lamda_SP))\n",
    "plt.xlabel(\"Predicted Sale Price\", fontsize=14)\n",
    "plt.ylabel(\"Actual Sale Price\", fontsize=14)\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "1. We performed an exploratory data analysis on the Ames Housing dataset and discovered some correlations between the predictors and the regressors. Box-cox transformation was used to modified the distribution of the features and the response variable for getting better predictive power. By engineering Ames dataset through 6 key steps, which include filling up the missing values with reasonable choices and applying one-hot encoding / label encoding to categorical variables, we were able to get a R^2 of 0.88-0.93 from various predictive models on the test set.\n",
    "2. We can use K-Fold cross-validation to further select the best hyperparameter for a better fit. Additionally, other models, e.g., XGBoost, SVM, and more sophisticated ensemble methods, e.g., stacking, can be applied to get a better fit of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible next steps:\n",
    "For AVM part, I would (1) look into stacking method, (2) remove some of the highly correlated features, e.g., leave only one of garage.car or garage.area in the data by inspection or by principal component analysis (PCA), and (3) do a grid search for hyperparameters to improve model performance. For the UI part, I would attempt to build a ModelSelection template for fine tuning the model-training process, and to build the preprocessing script so that all raw data can be filled in to a array of fixed size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
